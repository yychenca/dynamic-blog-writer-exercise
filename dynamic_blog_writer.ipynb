{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Dynamic Micro-Blog Writer\n## Multi-Agent System for Autonomous Blog Generation\n\n**Optimized for Google Colab with REAL Web Search**\n\nThis notebook implements a multi-agent system that autonomously generates high-quality blog posts through:\n1. **Persona Architect**: Creates writer persona and search queries\n2. **Research Analyst**: Performs REAL web searches (with options!)\n3. **Content Synthesizer**: Writes blog posts based on actual web research\n4. **Critic/Editor**: Reviews and provides feedback for iterative improvement\n\n**Author**: Student Developer  \n**Date**: August 12, 2025  \n**Version**: 2.0 (Real Web Search Edition)\n\n## Setup Options:\n\n### Option 1: FREE Version (Recommended)\n- **Required**: Only `GEMINI_API_KEY` \n- **Search**: DuckDuckGo (no additional API keys)\n- **Cost**: Completely FREE\n- **Setup**: Just add Gemini API key to Colab secrets\n\n### Option 2: Premium Version  \n- **Required**: `GEMINI_API_KEY` + `GOOGLE_CSE_API_KEY` + `GOOGLE_CSE_ID`\n- **Search**: Google Custom Search API\n- **Cost**: 100 searches/day FREE, then $5/1000 searches\n- **Setup**: Additional Google Custom Search setup required\n\n## Quick Setup for FREE Version:\n1. Click the key icon in the left sidebar\n2. Add secret: `GEMINI_API_KEY` (get from https://ai.google.dev/)\n3. Enable notebook access\n4. Run all cells!\n\n## What You Get:\n- **Real Web Search**: Actual current information from the web\n- **Live Data**: Blog content based on real-time search results  \n- **Source Tracking**: Shows actual URLs and snippets\n- **Rich Previews**: Beautiful markdown display of all content\n- **Current Information**: No longer limited to training data"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport json\nimport datetime\nimport re\nimport requests\nimport time\nfrom typing import Dict, List, Optional, Tuple\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom urllib.parse import quote_plus\n\ntry:\n    import google.generativeai as genai\nexcept ImportError:\n    print(\"Installing google-generativeai...\")\n    !pip install google-generativeai\n    import google.generativeai as genai\n\ntry:\n    from bs4 import BeautifulSoup\nexcept ImportError:\n    print(\"Installing beautifulsoup4 for web scraping...\")\n    !pip install beautifulsoup4\n    from bs4 import BeautifulSoup\n\n# Import IPython display for rich markdown rendering\nfrom IPython.display import display, Markdown\n\nprint(\"All dependencies loaded successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure APIs for Google Colab\n# Using Google Colab secrets for secure API key management\n\n# Choose your search method\nUSE_FREE_SEARCH = True  # Set to False to use Google Custom Search API\n\ntry:\n    # Import Google Colab userdata for secrets\n    from google.colab import userdata\n    \n    # Get Gemini API key (required)\n    gemini_api_key = userdata.get('GEMINI_API_KEY')\n    if not gemini_api_key:\n        raise ValueError(\"GEMINI_API_KEY not found in Colab secrets.\")\n    \n    # Configure Gemini\n    genai.configure(api_key=gemini_api_key)\n    model = genai.GenerativeModel('gemini-2.5-pro')\n    \n    if USE_FREE_SEARCH:\n        # Free web search - no additional API keys needed\n        GOOGLE_CSE_API_KEY = None\n        GOOGLE_CSE_ID = None\n        print(\"Gemini Pro 2.5 model initialized successfully!\")\n        print(\"Using FREE web search (no additional API keys required)\")\n        print(\"Only Gemini API key loaded from Colab secrets\")\n    else:\n        # Google Custom Search API (requires additional keys)\n        google_cse_api_key = userdata.get('GOOGLE_CSE_API_KEY')\n        google_cse_id = userdata.get('GOOGLE_CSE_ID')\n        \n        if not google_cse_api_key or not google_cse_id:\n            print(\"Google Custom Search keys not found, falling back to free search\")\n            USE_FREE_SEARCH = True\n            GOOGLE_CSE_API_KEY = None\n            GOOGLE_CSE_ID = None\n        else:\n            GOOGLE_CSE_API_KEY = google_cse_api_key\n            GOOGLE_CSE_ID = google_cse_id\n            print(\"Gemini Pro 2.5 model initialized successfully!\")\n            print(\"Google Custom Search API configured successfully!\")\n            print(\"API keys loaded from Google Colab secrets\")\n    \nexcept ImportError:\n    # Fallback for non-Colab environments\n    print(\"Not running in Google Colab, using fallback method...\")\n    \n    gemini_api_key = os.environ.get('GEMINI_API_KEY')\n    if not gemini_api_key:\n        gemini_api_key = input(\"Enter your Gemini API key: \")\n    \n    genai.configure(api_key=gemini_api_key)\n    model = genai.GenerativeModel('gemini-2.5-pro')\n    \n    if USE_FREE_SEARCH:\n        GOOGLE_CSE_API_KEY = None\n        GOOGLE_CSE_ID = None\n        print(\"Using FREE web search method\")\n    else:\n        google_cse_api_key = os.environ.get('GOOGLE_CSE_API_KEY')\n        google_cse_id = os.environ.get('GOOGLE_CSE_ID')\n        \n        if not google_cse_api_key:\n            google_cse_api_key = input(\"Enter your Google Custom Search API key (or press Enter for free search): \")\n        if not google_cse_id:\n            google_cse_id = input(\"Enter your Google Custom Search Engine ID (or press Enter for free search): \")\n        \n        if not google_cse_api_key or not google_cse_id:\n            USE_FREE_SEARCH = True\n            GOOGLE_CSE_API_KEY = None\n            GOOGLE_CSE_ID = None\n            print(\"Using FREE web search method\")\n        else:\n            GOOGLE_CSE_API_KEY = google_cse_api_key\n            GOOGLE_CSE_ID = google_cse_id\n            print(\"Using Google Custom Search API\")\n    \n    print(\"Gemini Pro 2.5 model initialized successfully!\")\n    \nexcept Exception as e:\n    print(f\"Error configuring APIs: {e}\")\n    print(\"Required setup:\")\n    print(\"   1. GEMINI_API_KEY - Your Gemini API key (REQUIRED)\")\n    if not USE_FREE_SEARCH:\n        print(\"   2. GOOGLE_CSE_API_KEY - Your Google Custom Search API key (OPTIONAL)\")\n        print(\"   3. GOOGLE_CSE_ID - Your Google Custom Search Engine ID (OPTIONAL)\")\n    print(\"\\nSetup instructions:\")\n    print(\"   • Gemini API: https://ai.google.dev/\")\n    if not USE_FREE_SEARCH:\n        print(\"   • Google Custom Search: https://developers.google.com/custom-search/v1/overview\")\n    raise"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamental Principles of Quality Writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUALITY_PRINCIPLES = \"\"\"\n",
    "FUNDAMENTAL PRINCIPLES OF QUALITY WRITING:\n",
    "\n",
    "P1: Evidentiary Support\n",
    "All claims must be directly traceable to the provided research material.\n",
    "\n",
    "P2: Clarity and Conciseness\n",
    "Writing must be precise, unambiguous, and free of unnecessary jargon.\n",
    "\n",
    "P3: Engaging Narrative\n",
    "The post must feature a strong hook, a logical flow, and a memorable conclusion.\n",
    "\n",
    "P4: Structural Integrity\n",
    "The output must be well-organized with a clear title, introduction, body, and conclusion.\n",
    "\n",
    "P5: Intellectual Honesty\n",
    "Information must be represented accurately, even when adopting a specific persona.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Quality principles defined:\")\n",
    "print(QUALITY_PRINCIPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Classes and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@dataclass\nclass SearchResult:\n    title: str\n    url: str\n    snippet: str\n\n@dataclass\nclass PersonaResult:\n    persona_prompt: str\n    search_queries: List[str]\n\n@dataclass\nclass EditorPersonaResult:\n    editor_persona: str\n    review_criteria: str\n\n@dataclass\nclass RequirementAnalysis:\n    persona_requirements: str  # For writer persona (tone, style, voice)\n    content_requirements: str  # For blog writing (length, structure, format)\n    review_requirements: str   # For editor review (standards, criteria)\n\n@dataclass\nclass ResearchResult:\n    content: str\n    source_count: int = 0\n    search_results: List[SearchResult] = None\n\n@dataclass\nclass BlogDraft:\n    content: str\n    version: int\n\n@dataclass\nclass EditorReview:\n    is_approved: bool\n    comments: str\n\n@dataclass\nclass FactualClaim:\n    claim_text: str\n    claim_type: str  # \"statistic\", \"date\", \"name\", \"fact\", \"quote\"\n    importance: str  # \"high\", \"medium\", \"low\"\n    context: str  # surrounding text for context\n    \n@dataclass\nclass ClaimVerification:\n    claim: FactualClaim\n    search_queries: List[str]\n    evidence: List[SearchResult]\n    verification_status: str  # \"verified\", \"contradicted\", \"insufficient_evidence\", \"unverifiable\"\n    confidence: float  # 0.0 to 1.0\n    explanation: str\n\n@dataclass\nclass FactCheckReport:\n    total_claims: int\n    verified_claims: int\n    contradicted_claims: int\n    insufficient_evidence_claims: int\n    overall_reliability: str  # \"high\", \"medium\", \"low\", \"unreliable\"\n    critical_issues: List[str]\n    verifications: List[ClaimVerification]\n\nclass ClaimExtractor:\n    \"\"\"Extract verifiable factual claims from blog content using LLM.\"\"\"\n    \n    def __init__(self, model):\n        self.model = model\n    \n    def extract_claims(self, blog_content: str, topic: str) -> List[FactualClaim]:\n        \"\"\"Extract verifiable factual claims from the blog post.\"\"\"\n        prompt = f\"\"\"\nYou are a Claim Extraction specialist. Your task is to identify and extract verifiable factual claims from a blog post.\n\nBLOG CONTENT:\n{blog_content}\n\nTOPIC: {topic}\n\nTASK: Extract factual claims that can be verified through online search. Focus on:\n\n1. STATISTICS AND NUMBERS: Specific percentages, amounts, quantities, growth rates\n2. DATES AND TIMEFRAMES: When events occurred, publication dates, deadlines\n3. NAMES AND ENTITIES: Companies, people, organizations, products, technologies\n4. SPECIFIC FACTS: Technical specifications, study results, market data\n5. QUOTES AND CITATIONS: Direct quotes, study findings, expert statements\n\nIGNORE:\n- Opinions and subjective statements\n- General trends without specific data\n- Common knowledge facts\n- Vague or unspecific claims\n\nFor each claim, determine:\n- claim_type: \"statistic\", \"date\", \"name\", \"fact\", \"quote\"\n- importance: \"high\" (core to article), \"medium\" (supporting), \"low\" (minor detail)\n\nOutput as JSON array:\n[\n  {{\n    \"claim_text\": \"Exact text of the claim\",\n    \"claim_type\": \"statistic|date|name|fact|quote\",\n    \"importance\": \"high|medium|low\",\n    \"context\": \"Surrounding sentence(s) for context\"\n  }}\n]\n\nExtract 5-15 most important verifiable claims. Prioritize claims that are:\n1. Central to the article's argument\n2. Specific and measurable\n3. Recent or time-sensitive\n4. About well-known entities/topics\n\nReturn ONLY the JSON array.\n\"\"\"\n        \n        try:\n            response = self.model.generate_content(prompt)\n            response_text = response.text.strip()\n            \n            # Clean JSON response\n            if response_text.startswith('```json'):\n                response_text = response_text[7:-3].strip()\n            elif response_text.startswith('```'):\n                response_text = response_text[3:-3].strip()\n            \n            claims_data = json.loads(response_text)\n            \n            claims = []\n            for claim_data in claims_data:\n                claim = FactualClaim(\n                    claim_text=claim_data.get('claim_text', ''),\n                    claim_type=claim_data.get('claim_type', 'fact'),\n                    importance=claim_data.get('importance', 'medium'),\n                    context=claim_data.get('context', '')\n                )\n                claims.append(claim)\n            \n            return claims\n            \n        except Exception as e:\n            print(f\"Error extracting claims: {e}\")\n            return []\n\nclass EvidenceEvaluator:\n    \"\"\"Evaluate factual claims against search evidence.\"\"\"\n    \n    def __init__(self, model):\n        self.model = model\n    \n    def evaluate_claim(self, claim: FactualClaim, evidence: List[SearchResult]) -> ClaimVerification:\n        \"\"\"Evaluate a single claim against search evidence.\"\"\"\n        \n        if not evidence:\n            return ClaimVerification(\n                claim=claim,\n                search_queries=[],\n                evidence=evidence,\n                verification_status=\"insufficient_evidence\",\n                confidence=0.0,\n                explanation=\"No search results found to verify this claim.\"\n            )\n        \n        # Format evidence for analysis\n        evidence_text = \"\"\n        for i, result in enumerate(evidence, 1):\n            evidence_text += f\"SOURCE {i}:\\n\"\n            evidence_text += f\"Title: {result.title}\\n\"\n            evidence_text += f\"URL: {result.url}\\n\" \n            evidence_text += f\"Content: {result.snippet}\\n\\n\"\n        \n        prompt = f\"\"\"\nYou are a Fact Verification specialist. Analyze whether a factual claim is supported by search evidence.\n\nCLAIM TO VERIFY:\n\"{claim.claim_text}\"\n\nCLAIM TYPE: {claim.claim_type}\nCLAIM IMPORTANCE: {claim.importance}\nCONTEXT: {claim.context}\n\nSEARCH EVIDENCE:\n{evidence_text}\n\nTASK: Determine if the claim is supported by the evidence.\n\nVERIFICATION CATEGORIES:\n1. \"verified\" - Evidence clearly supports the claim\n2. \"contradicted\" - Evidence contradicts the claim  \n3. \"insufficient_evidence\" - Not enough evidence to verify\n4. \"unverifiable\" - Claim is too vague or subjective to verify\n\nANALYSIS CRITERIA:\n- Are the sources credible and recent?\n- Do multiple sources support the claim?\n- Are the numbers/facts exactly matching?\n- Is the context similar?\n\nConsider source credibility, recency, and consistency across sources.\n\nOutput as JSON:\n{{\n  \"verification_status\": \"verified|contradicted|insufficient_evidence|unverifiable\",\n  \"confidence\": 0.8,\n  \"explanation\": \"Detailed explanation of why this verification status was assigned, citing specific evidence sources and any discrepancies found.\"\n}}\n\nBe thorough and conservative in verification. When in doubt, use \"insufficient_evidence\".\n\nReturn ONLY the JSON object.\n\"\"\"\n        \n        try:\n            response = self.model.generate_content(prompt)\n            response_text = response.text.strip()\n            \n            # Clean JSON response\n            if response_text.startswith('```json'):\n                response_text = response_text[7:-3].strip()\n            elif response_text.startswith('```'):\n                response_text = response_text[3:-3].strip()\n            \n            result_data = json.loads(response_text)\n            \n            return ClaimVerification(\n                claim=claim,\n                search_queries=[],  # Will be filled by FactChecker\n                evidence=evidence,\n                verification_status=result_data.get('verification_status', 'insufficient_evidence'),\n                confidence=float(result_data.get('confidence', 0.0)),\n                explanation=result_data.get('explanation', '')\n            )\n            \n        except Exception as e:\n            print(f\"Error evaluating claim: {e}\")\n            return ClaimVerification(\n                claim=claim,\n                search_queries=[],\n                evidence=evidence,\n                verification_status=\"insufficient_evidence\",\n                confidence=0.0,\n                explanation=f\"Error during evaluation: {e}\"\n            )\n\nclass FactChecker:\n    \"\"\"Orchestrate comprehensive fact-checking of blog content.\"\"\"\n    \n    def __init__(self, model, web_searcher):\n        self.model = model\n        self.web_searcher = web_searcher\n        self.claim_extractor = ClaimExtractor(model)\n        self.evidence_evaluator = EvidenceEvaluator(model)\n    \n    def generate_search_queries(self, claim: FactualClaim) -> List[str]:\n        \"\"\"Generate targeted search queries for a factual claim.\"\"\"\n        prompt = f\"\"\"\nGenerate 2-3 specific search queries to verify this factual claim.\n\nCLAIM: \"{claim.claim_text}\"\nCLAIM TYPE: {claim.claim_type}\nCONTEXT: {claim.context}\n\nCreate search queries that are:\n1. Specific and targeted to the exact claim\n2. Likely to find authoritative sources\n3. Include key terms from the claim\n4. Vary in approach (direct search, context search, source search)\n\nFor statistics: Include exact numbers and context\nFor names: Include full names and relevant context  \nFor dates: Include specific dates and events\nFor facts: Include key technical terms\n\nReturn as JSON array:\n[\"query1\", \"query2\", \"query3\"]\n\nReturn ONLY the JSON array.\n\"\"\"\n        \n        try:\n            response = self.model.generate_content(prompt)\n            response_text = response.text.strip()\n            \n            if response_text.startswith('```json'):\n                response_text = response_text[7:-3].strip()\n            elif response_text.startswith('```'):\n                response_text = response_text[3:-3].strip()\n            \n            queries = json.loads(response_text)\n            return queries if isinstance(queries, list) else [str(queries)]\n            \n        except Exception as e:\n            print(f\"Error generating search queries: {e}\")\n            # Fallback: create basic query from claim text\n            return [claim.claim_text]\n    \n    def fact_check_article(self, blog_content: str, topic: str) -> FactCheckReport:\n        \"\"\"Perform comprehensive fact-checking of the entire article.\"\"\"\n        print(\"   Starting comprehensive fact-checking...\")\n        \n        # Step 1: Extract factual claims\n        print(\"   Extracting factual claims...\")\n        claims = self.claim_extractor.extract_claims(blog_content, topic)\n        print(f\"   Found {len(claims)} factual claims to verify\")\n        \n        if not claims:\n            return FactCheckReport(\n                total_claims=0,\n                verified_claims=0,\n                contradicted_claims=0,\n                insufficient_evidence_claims=0,\n                overall_reliability=\"unknown\",\n                critical_issues=[\"No verifiable claims found in the article\"],\n                verifications=[]\n            )\n        \n        # Step 2: Prioritize claims (check high importance first)\n        high_priority_claims = [c for c in claims if c.importance == \"high\"]\n        medium_priority_claims = [c for c in claims if c.importance == \"medium\"]\n        low_priority_claims = [c for c in claims if c.importance == \"low\"]\n        \n        prioritized_claims = high_priority_claims + medium_priority_claims + low_priority_claims\n        \n        # Step 3: Verify each claim\n        verifications = []\n        verified_count = 0\n        contradicted_count = 0\n        insufficient_evidence_count = 0\n        \n        for i, claim in enumerate(prioritized_claims, 1):\n            print(f\"   Fact-checking claim {i}/{len(prioritized_claims)}: {claim.claim_text[:50]}...\")\n            \n            # Generate search queries\n            search_queries = self.generate_search_queries(claim)\n            \n            # Search for evidence\n            evidence = []\n            for query in search_queries:\n                query_results = self.web_searcher.search(query, num_results=5)\n                evidence.extend(query_results)\n            \n            # Remove duplicates\n            unique_evidence = []\n            seen_urls = set()\n            for result in evidence:\n                if result.url not in seen_urls:\n                    unique_evidence.append(result)\n                    seen_urls.add(result.url)\n            \n            # Evaluate claim against evidence\n            verification = self.evidence_evaluator.evaluate_claim(claim, unique_evidence[:8])  # Top 8 unique results\n            verification.search_queries = search_queries\n            \n            verifications.append(verification)\n            \n            # Update counters\n            if verification.verification_status == \"verified\":\n                verified_count += 1\n            elif verification.verification_status == \"contradicted\":\n                contradicted_count += 1\n            else:\n                insufficient_evidence_count += 1\n        \n        # Step 4: Generate overall assessment\n        critical_issues = []\n        contradicted_verifications = [v for v in verifications if v.verification_status == \"contradicted\"]\n        \n        for verification in contradicted_verifications:\n            if verification.claim.importance == \"high\":\n                critical_issues.append(f\"HIGH PRIORITY: Contradicted claim - {verification.claim.claim_text}\")\n            elif verification.claim.importance == \"medium\":\n                critical_issues.append(f\"MEDIUM PRIORITY: Contradicted claim - {verification.claim.claim_text}\")\n        \n        # Determine overall reliability\n        if contradicted_count > 0:\n            if any(v.claim.importance == \"high\" for v in contradicted_verifications):\n                overall_reliability = \"unreliable\"\n            else:\n                overall_reliability = \"low\"\n        elif verified_count >= len(claims) * 0.7:  # 70%+ verified\n            overall_reliability = \"high\"\n        elif verified_count >= len(claims) * 0.4:  # 40%+ verified  \n            overall_reliability = \"medium\"\n        else:\n            overall_reliability = \"low\"\n        \n        print(f\"   Fact-checking complete: {verified_count} verified, {contradicted_count} contradicted, {insufficient_evidence_count} insufficient evidence\")\n        \n        return FactCheckReport(\n            total_claims=len(claims),\n            verified_claims=verified_count,\n            contradicted_claims=contradicted_count,\n            insufficient_evidence_claims=insufficient_evidence_count,\n            overall_reliability=overall_reliability,\n            critical_issues=critical_issues,\n            verifications=verifications\n        )\n\nprint(\"Advanced fact-checking system with claim extraction and evidence evaluation implemented successfully!\")"
  },
  {
   "cell_type": "code",
   "source": "class RequirementAnalyzer:\n    \"\"\"Intelligently analyze and categorize style_and_background requirements using LLM.\"\"\"\n    \n    def __init__(self, model):\n        self.model = model\n    \n    def analyze_requirements(self, style_and_background: str, topic: str) -> RequirementAnalysis:\n        \"\"\"Use LLM to intelligently categorize requirements.\"\"\"\n        prompt = f\"\"\"\nYou are a Requirements Analyzer. Analyze the following style and background requirements and intelligently categorize them for different purposes.\n\nTOPIC: {topic}\n\nSTYLE & BACKGROUND REQUIREMENTS:\n{style_and_background}\n\nTASK: Categorize these requirements into three specific areas:\n\n1. PERSONA REQUIREMENTS (for writer persona generation):\n   - Tone and voice characteristics\n   - Writing style preferences\n   - Personality traits\n   - Perspective and approach\n   - Expertise level to portray\n\n2. CONTENT REQUIREMENTS (for blog writing process):\n   - Length specifications (word counts, article length)\n   - Structural requirements (format, sections, organization)\n   - Content depth and complexity\n   - Specific elements to include/exclude\n   - Technical specifications\n\n3. REVIEW REQUIREMENTS (for editorial review):\n   - Quality standards and criteria\n   - Accuracy requirements\n   - Audience appropriateness checks\n   - Compliance standards\n   - Validation criteria\n\nIMPORTANT:\n- Extract and categorize ALL relevant information from the requirements\n- Don't duplicate information across categories\n- Be comprehensive but avoid redundancy\n- If a requirement fits multiple categories, place it in the most appropriate one\n- Convert implicit requirements into explicit instructions\n- Maintain the original intent and specificity\n\nOutput as JSON:\n{{\n  \"persona_requirements\": \"Detailed requirements for persona generation...\",\n  \"content_requirements\": \"Detailed requirements for content creation...\",\n  \"review_requirements\": \"Detailed requirements for editorial review...\"\n}}\n\nReturn ONLY the JSON object.\n\"\"\"\n        \n        try:\n            response = self.model.generate_content(prompt)\n            response_text = response.text.strip()\n            \n            # Clean JSON response\n            if response_text.startswith('```json'):\n                response_text = response_text[7:-3].strip()\n            elif response_text.startswith('```'):\n                response_text = response_text[3:-3].strip()\n            \n            data = json.loads(response_text)\n            \n            return RequirementAnalysis(\n                persona_requirements=data.get('persona_requirements', ''),\n                content_requirements=data.get('content_requirements', ''),\n                review_requirements=data.get('review_requirements', '')\n            )\n            \n        except Exception as e:\n            print(f\"Requirement analysis error: {e}\")\n            # Fallback to original requirements\n            return RequirementAnalysis(\n                persona_requirements=style_and_background,\n                content_requirements=style_and_background,\n                review_requirements=style_and_background\n            )\n\nclass FolderNameGenerator:\n    \"\"\"Generate smart folder names using LLM.\"\"\"\n    \n    def __init__(self, model):\n        self.model = model\n    \n    def generate_keywords(self, topic: str) -> Tuple[str, str]:\n        \"\"\"Generate 2 keywords for folder naming.\"\"\"\n        prompt = f\"\"\"\n        Generate exactly 2 keywords that best represent this topic for folder naming.\n        \n        TOPIC: {topic}\n        \n        REQUIREMENTS:\n        1. Return exactly 2 keywords\n        2. Keywords should be lowercase\n        3. Keywords should be single words (no spaces, hyphens, or special characters)\n        4. Keywords should be relevant and descriptive\n        5. Use underscores to replace spaces if needed (but prefer single words)\n        6. Maximum 15 characters per keyword\n        \n        Return ONLY the keywords separated by a comma, like: keyword1,keyword2\n        \n        Examples:\n        - \"AI in Healthcare\" → \"ai,healthcare\"\n        - \"Climate Change Solutions\" → \"climate,solutions\"  \n        - \"Remote Work Productivity\" → \"remote,productivity\"\n        \"\"\"\n        \n        try:\n            response = self.model.generate_content(prompt)\n            keywords_text = response.text.strip().lower()\n            \n            # Clean and extract keywords\n            keywords = [k.strip() for k in keywords_text.split(',')]\n            \n            # Ensure we have exactly 2 keywords\n            if len(keywords) >= 2:\n                keyword1 = re.sub(r'[^a-z0-9_]', '', keywords[0])[:15]\n                keyword2 = re.sub(r'[^a-z0-9_]', '', keywords[1])[:15]\n                return keyword1, keyword2\n            else:\n                # Fallback to topic-based extraction\n                words = re.findall(r'\\\\w+', topic.lower())\n                return words[0][:15] if words else \"topic\", words[1][:15] if len(words) > 1 else \"blog\"\n                \n        except Exception as e:\n            print(f\"Keyword generation error: {e}\")\n            # Fallback to simple extraction\n            words = re.findall(r'\\\\w+', topic.lower())\n            return words[0][:15] if words else \"topic\", words[1][:15] if len(words) > 1 else \"blog\"\n\nclass FreeWebSearcher:\n    \"\"\"Free web search using DuckDuckGo search (no API key required).\"\"\"\n    \n    def __init__(self):\n        self.session = requests.Session()\n        self.session.headers.update({\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n        })\n    \n    def search(self, query: str, num_results: int = 10) -> List[SearchResult]:\n        \"\"\"Perform a free web search using DuckDuckGo.\"\"\"\n        try:\n            # Use DuckDuckGo search\n            search_url = f\"https://html.duckduckgo.com/html/?q={quote_plus(query)}\"\n            \n            response = self.session.get(search_url, timeout=10)\n            response.raise_for_status()\n            \n            soup = BeautifulSoup(response.content, 'html.parser')\n            results = []\n            \n            # Parse DuckDuckGo results\n            for result_div in soup.find_all('div', class_='result')[:num_results]:\n                try:\n                    title_elem = result_div.find('a', class_='result__a')\n                    snippet_elem = result_div.find('a', class_='result__snippet')\n                    \n                    if title_elem and snippet_elem:\n                        title = title_elem.get_text(strip=True)\n                        url = title_elem.get('href', '')\n                        snippet = snippet_elem.get_text(strip=True)\n                        \n                        if title and url and snippet:\n                            results.append(SearchResult(\n                                title=title,\n                                url=url,\n                                snippet=snippet\n                            ))\n                except Exception:\n                    continue\n            \n            return results\n            \n        except Exception as e:\n            print(f\"Free search error: {e}\")\n            return []\n    \n    def search_multiple_queries(self, queries: List[str], results_per_query: int = 10) -> List[SearchResult]:\n        \"\"\"Search multiple queries and return combined results.\"\"\"\n        all_results = []\n        \n        for i, query in enumerate(queries):\n            print(f\"   Free searching: '{query}'\")\n            results = self.search(query, results_per_query)\n            all_results.extend(results)\n            \n            # Add delay to be respectful\n            if i < len(queries) - 1:\n                time.sleep(2)\n        \n        return all_results\n\nclass WebSearcher:\n    def __init__(self, api_key: str = None, cse_id: str = None):\n        self.api_key = api_key\n        self.cse_id = cse_id\n        self.base_url = \"https://www.googleapis.com/customsearch/v1\"\n    \n    def search(self, query: str, num_results: int = 10) -> List[SearchResult]:\n        \"\"\"Perform a Google Custom Search and return top results.\"\"\"\n        try:\n            params = {\n                'key': self.api_key,\n                'cx': self.cse_id,\n                'q': query,\n                'num': min(num_results, 10)  # API max is 10 per request\n            }\n            \n            response = requests.get(self.base_url, params=params)\n            response.raise_for_status()\n            \n            data = response.json()\n            results = []\n            \n            if 'items' in data:\n                for item in data['items']:\n                    result = SearchResult(\n                        title=item.get('title', ''),\n                        url=item.get('link', ''),\n                        snippet=item.get('snippet', '')\n                    )\n                    results.append(result)\n            \n            return results\n            \n        except requests.exceptions.RequestException as e:\n            print(f\"Search API error: {e}\")\n            return []\n        except Exception as e:\n            print(f\"Unexpected search error: {e}\")\n            return []\n    \n    def search_multiple_queries(self, queries: List[str], results_per_query: int = 10) -> List[SearchResult]:\n        \"\"\"Search multiple queries and return combined results.\"\"\"\n        all_results = []\n        \n        for i, query in enumerate(queries):\n            print(f\"   Searching: '{query}'\")\n            results = self.search(query, results_per_query)\n            all_results.extend(results)\n            \n            # Add delay to respect API rate limits\n            if i < len(queries) - 1:\n                time.sleep(1)\n        \n        return all_results\n\nclass FileManager:\n    def __init__(self, topic: str, folder_name_generator: FolderNameGenerator):\n        self.topic = topic\n        self.date_str = datetime.datetime.now().strftime(\"%Y%m%d\")\n        \n        # Generate smart keywords for folder naming\n        keyword1, keyword2 = folder_name_generator.generate_keywords(topic)\n        self.folder_name = f\"{self.date_str}_{keyword1}_{keyword2}\"\n        self.output_dir = Path(self.folder_name)\n        self.output_dir.mkdir(exist_ok=True)\n        print(f\"Created output directory: {self.output_dir}\")\n    \n    def save_draft(self, content: str, version: int) -> Path:\n        filename = f\"draft_{version}.md\"\n        filepath = self.output_dir / filename\n        with open(filepath, 'w', encoding='utf-8') as f:\n            f.write(content)\n        print(f\"Saved draft to: {filepath}\")\n        return filepath\n    \n    def save_review(self, review: EditorReview, version: int) -> Path:\n        filename = f\"review_{version}.md\"\n        filepath = self.output_dir / filename\n        content = f\"# Editorial Review {version}\\\\n\\\\n\"\n        content += f\"**Approved:** {review.is_approved}\\\\n\\\\n\"\n        content += f\"**Comments:**\\\\n{review.comments}\\\\n\"\n        with open(filepath, 'w', encoding='utf-8') as f:\n            f.write(content)\n        print(f\"Saved review to: {filepath}\")\n        return filepath\n    \n    def save_final_blog(self, content: str) -> Path:\n        filepath = self.output_dir / \"final_blog.md\"\n        with open(filepath, 'w', encoding='utf-8') as f:\n            f.write(content)\n        print(f\"Saved final blog to: {filepath}\")\n        return filepath\n    \n    def save_persona_details(self, persona_prompt: str, topic: str) -> Path:\n        filename = \"writer_persona.md\"\n        filepath = self.output_dir / filename\n        content = f\"# Writer Persona for: {topic}\\\\n\\\\n\"\n        content += f\"**Generated on:** {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\\\n\\\\n\"\n        content += f\"## Writer Persona Details\\\\n{persona_prompt}\\\\n\"\n        with open(filepath, 'w', encoding='utf-8') as f:\n            f.write(content)\n        print(f\"Saved writer persona to: {filepath}\")\n        return filepath\n    \n    def save_editor_persona_details(self, editor_persona: str, review_criteria: str, topic: str) -> Path:\n        filename = \"editor_persona.md\"\n        filepath = self.output_dir / filename\n        content = f\"# Editor Persona for: {topic}\\\\n\\\\n\"\n        content += f\"**Generated on:** {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\\\n\\\\n\"\n        content += f\"## Editor Persona Details\\\\n{editor_persona}\\\\n\\\\n\"\n        content += f\"## Review Criteria\\\\n{review_criteria}\\\\n\"\n        with open(filepath, 'w', encoding='utf-8') as f:\n            f.write(content)\n        print(f\"Saved editor persona to: {filepath}\")\n        return filepath\n    \n    def save_search_results(self, search_results: List[SearchResult]) -> Path:\n        filename = \"search_results.md\"\n        filepath = self.output_dir / filename\n        content = f\"# Search Results\\\\n\\\\n\"\n        content += f\"**Generated on:** {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\\\n\"\n        content += f\"**Total Results:** {len(search_results)}\\\\n\\\\n\"\n        \n        for i, result in enumerate(search_results, 1):\n            content += f\"## Result {i}\\\\n\\\\n\"\n            content += f\"**Title:** {result.title}\\\\n\\\\n\"\n            content += f\"**URL:** {result.url}\\\\n\\\\n\"\n            content += f\"**Snippet:** {result.snippet}\\\\n\\\\n\"\n            content += \"---\\\\n\\\\n\"\n        \n        with open(filepath, 'w', encoding='utf-8') as f:\n            f.write(content)\n        print(f\"Saved search results to: {filepath}\")\n        return filepath\n    \n    def save_requirement_analysis(self, analysis: RequirementAnalysis, topic: str) -> Path:\n        filename = \"requirement_analysis.md\"\n        filepath = self.output_dir / filename\n        content = f\"# Requirement Analysis for: {topic}\\\\n\\\\n\"\n        content += f\"**Generated on:** {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\\\n\\\\n\"\n        content += f\"## Persona Requirements\\\\n{analysis.persona_requirements}\\\\n\\\\n\"\n        content += f\"## Content Requirements\\\\n{analysis.content_requirements}\\\\n\\\\n\"\n        content += f\"## Review Requirements\\\\n{analysis.review_requirements}\\\\n\"\n        with open(filepath, 'w', encoding='utf-8') as f:\n            f.write(content)\n        print(f\"Saved requirement analysis to: {filepath}\")\n        return filepath\n\nprint(\"Enhanced utility classes with web searcher implementations defined successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Add missing save_fact_check_report method to FileManager\ndef save_fact_check_report(self, fact_check_report, version: int):\n    \"\"\"Save comprehensive fact-check report.\"\"\"\n    filename = f\"fact_check_{version}.md\"\n    filepath = self.output_dir / filename\n    \n    content = f\"# Fact-Check Report {version}\\\\n\\\\n\"\n    content += f\"**Generated on:** {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\\\n\\\\n\"\n    \n    # Summary\n    content += f\"## Summary\\\\n\\\\n\"\n    content += f\"- **Total Claims Checked:** {fact_check_report.total_claims}\\\\n\"\n    content += f\"- **Verified Claims:** {fact_check_report.verified_claims}\\\\n\"\n    content += f\"- **Contradicted Claims:** {fact_check_report.contradicted_claims}\\\\n\"\n    content += f\"- **Insufficient Evidence:** {fact_check_report.insufficient_evidence_claims}\\\\n\"\n    content += f\"- **Overall Reliability:** {fact_check_report.overall_reliability.upper()}\\\\n\\\\n\"\n    \n    # Critical Issues\n    if fact_check_report.critical_issues:\n        content += f\"## Critical Issues\\\\n\\\\n\"\n        for issue in fact_check_report.critical_issues:\n            content += f\"- {issue}\\\\n\"\n        content += \"\\\\n\"\n    else:\n        content += f\"## Critical Issues\\\\n\\\\nNone detected.\\\\n\\\\n\"\n    \n    # Detailed Results\n    content += f\"## Detailed Verification Results\\\\n\\\\n\"\n    for i, verification in enumerate(fact_check_report.verifications, 1):\n        content += f\"### Claim {i}\\\\n\\\\n\"\n        content += f\"**Claim:** {verification.claim.claim_text}\\\\n\\\\n\"\n        content += f\"**Type:** {verification.claim.claim_type}\\\\n\"\n        content += f\"**Importance:** {verification.claim.importance}\\\\n\"\n        content += f\"**Status:** {verification.verification_status.upper()}\\\\n\"\n        content += f\"**Confidence:** {verification.confidence:.2f}\\\\n\\\\n\"\n        content += f\"**Context:** {verification.claim.context}\\\\n\\\\n\"\n        content += f\"**Explanation:** {verification.explanation}\\\\n\\\\n\"\n        \n        if verification.search_queries:\n            content += f\"**Search Queries Used:**\\\\n\"\n            for query in verification.search_queries:\n                content += f\"- {query}\\\\n\"\n            content += \"\\\\n\"\n        \n        if verification.evidence:\n            content += f\"**Evidence Sources ({len(verification.evidence)}):**\\\\n\"\n            for j, evidence in enumerate(verification.evidence[:5], 1):  # Top 5 sources\n                content += f\"{j}. **{evidence.title}**\\\\n\"\n                content += f\"   URL: {evidence.url}\\\\n\"\n                content += f\"   Summary: {evidence.snippet}\\\\n\\\\n\"\n            \n            if len(verification.evidence) > 5:\n                content += f\"   ... and {len(verification.evidence) - 5} more sources\\\\n\"\n        \n        content += \"---\\\\n\\\\n\"\n    \n    with open(filepath, 'w', encoding='utf-8') as f:\n        f.write(content)\n    print(f\"Saved fact-check report to: {filepath}\")\n    return filepath\n\n# Patch the FileManager class to include the missing method\nFileManager.save_fact_check_report = save_fact_check_report\n\nprint(\"FileManager patched with save_fact_check_report method!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Implementation: Persona Architect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class PersonaArchitect:\n    def __init__(self, model):\n        self.model = model\n    \n    def generate_persona_and_queries(self, topic: str, persona_requirements: str) -> PersonaResult:\n        \"\"\"Generate persona using intelligently analyzed requirements.\"\"\"\n        prompt = f\"\"\"\nYou are a Persona Architect agent. Your task is to create a detailed writer persona and search queries for a blog post.\n\nTOPIC: {topic}\n\nPERSONA-SPECIFIC REQUIREMENTS:\n{persona_requirements}\n\nREQUIREMENTS:\n1. Generate a detailed, topic-specific writer persona based on the provided persona requirements\n2. Generate at least 3 optimized web search queries relevant to the topic\n3. Output must be a valid JSON object with:\n   - \"persona_prompt\": detailed writer persona description\n   - \"search_queries\": list of at least 3 search query strings\n\nThe persona should include:\n- Professional background and expertise (based on requirements)\n- Writing style and tone (as specified in requirements)\n- Target audience understanding (from requirements)\n- Unique perspective or angle (guided by requirements)\n- Personality traits and approach (from requirements)\n\nSearch queries should be:\n- Specific and targeted to the topic\n- Diverse in scope (different angles/aspects)\n- Optimized for finding current, reliable information\n- Aligned with the persona's expertise level\n\nIMPORTANT: Focus on the persona-specific requirements provided. These have been intelligently extracted and categorized for persona generation.\n\nReturn ONLY the JSON object, no additional text.\n\"\"\"\n        \n        try:\n            response = self.model.generate_content(prompt)\n            response_text = response.text.strip()\n            \n            # Clean the response to extract JSON\n            if response_text.startswith('```json'):\n                response_text = response_text[7:-3].strip()\n            elif response_text.startswith('```'):\n                response_text = response_text[3:-3].strip()\n            \n            # Debug: Print the response for troubleshooting\n            print(f\"DEBUG - Persona generation response length: {len(response_text)}\")\n            print(f\"DEBUG - First 100 chars: {response_text[:100]}\")\n            \n            data = json.loads(response_text)\n            \n            # Ensure we have the correct data types\n            persona_prompt = data.get('persona_prompt', '')\n            search_queries = data.get('search_queries', [])\n            \n            # Handle case where persona_prompt might be a dict or other type\n            if isinstance(persona_prompt, dict):\n                persona_prompt = str(persona_prompt)\n            elif not isinstance(persona_prompt, str):\n                persona_prompt = str(persona_prompt)\n                \n            # Ensure search_queries is a list\n            if not isinstance(search_queries, list):\n                search_queries = [str(search_queries)]\n            \n            return PersonaResult(\n                persona_prompt=persona_prompt,\n                search_queries=search_queries\n            )\n            \n        except json.JSONDecodeError as e:\n            print(f\"JSON parsing error in PersonaArchitect: {e}\")\n            print(f\"Raw response: {response_text}\")\n            raise\n        except Exception as e:\n            print(f\"Error in PersonaArchitect: {e}\")\n            print(f\"Raw response: {response.text if 'response' in locals() else 'No response'}\")\n            raise\n\nclass EditorPersonaArchitect:\n    \"\"\"Generate independent editor persona based on topic.\"\"\"\n    \n    def __init__(self, model):\n        self.model = model\n    \n    def generate_editor_persona(self, topic: str, review_requirements: str) -> EditorPersonaResult:\n        \"\"\"Generate editor persona using intelligently analyzed requirements.\"\"\"\n        prompt = f\"\"\"\nYou are an Editor Persona Architect. Create an independent editorial persona and review criteria.\n\nTOPIC: {topic}\n\nREVIEW-SPECIFIC REQUIREMENTS:\n{review_requirements}\n\nREQUIREMENTS:\n1. Generate an editor persona COMPLETELY INDEPENDENT from any writer persona\n2. The editor should be a topic expert with relevant editorial experience\n3. Create specific review criteria based on the review requirements provided\n4. Output must be a valid JSON object with:\n   - \"editor_persona\": detailed editor persona description\n   - \"review_criteria\": specific editorial standards and criteria\n\nThe editor persona should include:\n- Professional editorial background relevant to the topic\n- Subject matter expertise (independent perspective)\n- Editorial experience and credentials\n- Quality standards and expertise areas\n- Editorial philosophy and approach\n\nReview criteria should be based on:\n- The review requirements provided (intelligently analyzed)\n- Topic-specific accuracy requirements\n- Quality standards from requirements\n- Compliance and validation needs\n- Editorial best practices for this domain\n\nIMPORTANT: Use the review requirements provided - these have been intelligently extracted and categorized for editorial review purposes.\n\nReturn ONLY the JSON object, no additional text.\n\"\"\"\n        \n        try:\n            response = self.model.generate_content(prompt)\n            response_text = response.text.strip()\n            \n            # Clean the response to extract JSON\n            if response_text.startswith('```json'):\n                response_text = response_text[7:-3].strip()\n            elif response_text.startswith('```'):\n                response_text = response_text[3:-3].strip()\n            \n            data = json.loads(response_text)\n            \n            editor_persona = data.get('editor_persona', '')\n            review_criteria = data.get('review_criteria', '')\n            \n            # Ensure strings\n            if not isinstance(editor_persona, str):\n                editor_persona = str(editor_persona)\n            if not isinstance(review_criteria, str):\n                review_criteria = str(review_criteria)\n            \n            return EditorPersonaResult(\n                editor_persona=editor_persona,\n                review_criteria=review_criteria\n            )\n            \n        except Exception as e:\n            print(f\"Error in EditorPersonaArchitect: {e}\")\n            raise\n\nprint(\"Enhanced PersonaArchitect and EditorPersonaArchitect with intelligent requirements implemented successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Implementation: Research Analyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class ResearchAnalyst:\n    def __init__(self, model, web_searcher: WebSearcher):\n        self.model = model\n        self.web_searcher = web_searcher\n    \n    def conduct_research(self, search_queries: List[str]) -> ResearchResult:\n        \"\"\"Conduct real web research using Google Custom Search.\"\"\"\n        print(\"   Performing real web searches...\")\n        \n        # Perform actual web searches\n        search_results = self.web_searcher.search_multiple_queries(search_queries, results_per_query=10)\n        \n        if not search_results:\n            print(\"   No search results found, falling back to knowledge-based research\")\n            return self._fallback_research(search_queries)\n        \n        print(f\"   Found {len(search_results)} total search results\")\n        \n        # Prepare search content for analysis\n        search_content = self._format_search_results(search_results)\n        \n        # Use LLM to analyze and synthesize the search results\n        analysis_prompt = f\"\"\"\nYou are a Research Analyst. Analyze the following real web search results and create a comprehensive research summary.\n\nSEARCH QUERIES USED:\n{chr(10).join(f'- {query}' for query in search_queries)}\n\nSEARCH RESULTS TO ANALYZE:\n{search_content}\n\nTASK:\nAnalyze and synthesize these real search results into a comprehensive research summary. \n\nREQUIREMENTS:\n1. Extract key facts, statistics, and insights from the search results\n2. Identify recent developments and trends mentioned in the results\n3. Note expert opinions and authoritative sources\n4. Synthesize information from multiple sources\n5. Maintain accuracy to the source material\n6. Include relevant examples and case studies found\n\nFormat your response as:\n--- RESEARCH START ---\n[Your comprehensive analysis and synthesis of the search results]\n--- RESEARCH END ---\n\nFocus on creating a coherent narrative from the search results while preserving factual accuracy.\n\"\"\"\n        \n        try:\n            response = self.model.generate_content(analysis_prompt)\n            content = response.text.strip()\n            \n            return ResearchResult(\n                content=content, \n                source_count=len(search_results),\n                search_results=search_results\n            )\n            \n        except Exception as e:\n            print(f\"   Error analyzing search results: {e}\")\n            print(\"   Falling back to basic search result compilation\")\n            return self._compile_search_results(search_results)\n    \n    def _format_search_results(self, search_results: List[SearchResult]) -> str:\n        \"\"\"Format search results for LLM analysis.\"\"\"\n        formatted = \"\"\n        for i, result in enumerate(search_results, 1):\n            formatted += f\"\\\\n=== RESULT {i} ===\\\\n\"\n            formatted += f\"Title: {result.title}\\\\n\"\n            formatted += f\"URL: {result.url}\\\\n\"\n            formatted += f\"Content: {result.snippet}\\\\n\"\n        return formatted\n    \n    def _compile_search_results(self, search_results: List[SearchResult]) -> ResearchResult:\n        \"\"\"Compile search results into a basic research format.\"\"\"\n        content = f\"SOURCES CONSULTED: {len(search_results)} web sources\\\\n\\\\n\"\n        content += \"--- RESEARCH START ---\\\\n\"\n        content += \"Based on real web search results:\\\\n\\\\n\"\n        \n        for i, result in enumerate(search_results, 1):\n            content += f\"{i}. **{result.title}**\\\\n\"\n            content += f\"   Source: {result.url}\\\\n\"\n            content += f\"   Summary: {result.snippet}\\\\n\\\\n\"\n        \n        content += \"--- RESEARCH END ---\"\n        \n        return ResearchResult(\n            content=content,\n            source_count=len(search_results),\n            search_results=search_results\n        )\n    \n    def _fallback_research(self, search_queries: List[str]) -> ResearchResult:\n        \"\"\"Fallback to knowledge-based research if web search fails.\"\"\"\n        prompt = f\"\"\"\nYou are a Research Analyst. Web search is unavailable, so provide research based on your knowledge.\n\nSEARCH QUERIES:\n{chr(10).join(f'- {query}' for query in search_queries)}\n\nProvide comprehensive information based on your training data. Format as:\nSOURCES CONSULTED: Knowledge base (web search unavailable)\n\n--- RESEARCH START ---\n[Your research content here]\n--- RESEARCH END ---\n\"\"\"\n        \n        try:\n            response = self.model.generate_content(prompt)\n            content = response.text.strip()\n            return ResearchResult(content=content, source_count=0, search_results=[])\n        except Exception as e:\n            print(f\"   Fallback research failed: {e}\")\n            raise\n\nprint(\"ResearchAnalyst agent with real web search implemented successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Implementation: Content Synthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class ContentSynthesizer:\n    def __init__(self, model):\n        self.model = model\n    \n    def write_blog_post(self, persona_prompt: str, researched_content: str, \n                       topic: str, content_requirements: str, editorial_feedback: Optional[str] = None) -> BlogDraft:\n        \"\"\"Write blog post using intelligently analyzed content requirements.\"\"\"\n        \n        base_prompt = f\"\"\"\nYou are a Content Synthesizer agent. Your task is to write a high-quality blog post.\n\nWRITER PERSONA:\n{persona_prompt}\n\nRESEARCH CONTENT:\n{researched_content}\n\nTOPIC: {topic}\n\nCONTENT-SPECIFIC REQUIREMENTS:\n{content_requirements}\n\nFUNDAMENTAL PRINCIPLES TO FOLLOW:\n{QUALITY_PRINCIPLES}\n\nREQUIREMENTS:\n1. Follow ALL content requirements specified above (these have been intelligently analyzed and extracted)\n2. Use the writer persona voice and style exactly\n3. Adhere strictly to all Fundamental Principles (P1-P5)\n4. Include a compelling title, introduction, body, and conclusion\n5. Ensure all claims are traceable to the research content\n6. Make the post engaging with a strong hook and memorable conclusion\n\nIMPORTANT NOTES:\n- The content requirements above include length specifications, structural requirements, formatting needs, and other content-specific instructions\n- These requirements have been intelligently parsed and categorized specifically for content creation\n- Pay special attention to any length requirements, structural specifications, or formatting instructions\n- If length requirements are specified, ensure you meet or exceed them with substantive, valuable content\n\"\"\"\n        \n        if editorial_feedback:\n            base_prompt += f\"\"\"\n\nEDITORIAL FEEDBACK TO ADDRESS:\n{editorial_feedback}\n\nIMPORTANT: Specifically address the editor's comments while maintaining the quality principles AND the content requirements.\n\"\"\"\n        \n        base_prompt += \"\"\"\n\nWrite the blog post in Markdown format. Include the title as an H1 header.\nFocus on creating high-quality, valuable content that meets all specified requirements.\n\"\"\"\n        \n        try:\n            response = self.model.generate_content(base_prompt)\n            content = response.text.strip()\n            \n            # Determine version number based on whether this is a revision\n            version = 1 if editorial_feedback is None else 2\n            \n            return BlogDraft(content=content, version=version)\n            \n        except Exception as e:\n            print(f\"Error in ContentSynthesizer: {e}\")\n            raise\n\nprint(\"Enhanced ContentSynthesizer with intelligent content requirements implemented successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Implementation: Critic/Editor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class CriticEditor:\n    def __init__(self, model, web_searcher):\n        self.model = model\n        self.web_searcher = web_searcher\n        self.fact_checker = FactChecker(model, web_searcher)\n    \n    def _count_words(self, text: str) -> int:\n        \"\"\"Count words in the blog post content.\"\"\"\n        # Remove markdown formatting for accurate word count\n        clean_text = re.sub(r'[#*`\\-\\[\\]()]+', ' ', text)\n        clean_text = re.sub(r'https?://[^\\s]+', '', clean_text)\n        words = re.findall(r'\\b\\w+\\b', clean_text.lower())\n        return len(words)\n    \n    def review_draft(self, draft_blog: str, research_content: str, \n                    editor_persona: str, editor_review_criteria: str, \n                    content_requirements: str, topic: str) -> Tuple[EditorReview, FactCheckReport]:\n        \"\"\"Review draft with comprehensive fact-checking using intelligent requirement analysis.\"\"\"\n        \n        # Count words in the draft\n        word_count = self._count_words(draft_blog)\n        \n        print(\"  - Performing comprehensive fact-checking...\")\n        fact_check_report = self.fact_checker.fact_check_article(draft_blog, topic)\n        \n        # Determine if fact-checking passes\n        fact_check_passes = fact_check_report.overall_reliability in [\"high\", \"medium\"]\n        has_critical_issues = len(fact_check_report.critical_issues) > 0\n        \n        prompt = f\"\"\"\nYou are a Critic/Editor with the following persona and expertise:\n\nEDITOR PERSONA:\n{editor_persona}\n\nSPECIALIZED REVIEW CRITERIA:\n{editor_review_criteria}\n\nBLOG DRAFT TO REVIEW:\n{draft_blog}\n\nRESEARCH CONTENT FOR FACT-CHECKING:\n{research_content}\n\nCONTENT REQUIREMENTS TO VALIDATE:\n{content_requirements}\n\nFUNDAMENTAL PRINCIPLES TO EVALUATE AGAINST:\n{QUALITY_PRINCIPLES}\n\nCOMPREHENSIVE FACT-CHECK REPORT:\n- Total Claims Checked: {fact_check_report.total_claims}\n- Verified Claims: {fact_check_report.verified_claims}\n- Contradicted Claims: {fact_check_report.contradicted_claims}\n- Insufficient Evidence: {fact_check_report.insufficient_evidence_claims}\n- Overall Reliability: {fact_check_report.overall_reliability}\n- Critical Issues: {len(fact_check_report.critical_issues)}\n\nCRITICAL FACTUAL ISSUES FOUND:\n{chr(10).join(fact_check_report.critical_issues) if fact_check_report.critical_issues else \"None\"}\n\nDETAILED FACT-CHECK RESULTS:\n{self._format_fact_check_details(fact_check_report)}\n\nWORD COUNT ANALYSIS:\n- Current word count: {word_count} words\n\nEVALUATION CRITERIA:\n1. Check adherence to each fundamental principle (P1-P5)\n2. Verify factual consistency with research material\n3. CRITICAL: Assess fact-check results - articles with contradicted claims must be rejected\n4. Assess compliance with specialized review criteria\n5. Validate against ALL content requirements (including length, structure, format)\n6. Check topic-specific accuracy and expertise level\n7. Assess audience appropriateness\n8. Verify meeting of technical specifications\n\nFACT-CHECKING REQUIREMENTS:\n- Articles with \"unreliable\" fact-check rating must be REJECTED\n- Articles with contradicted high-priority claims must be REJECTED\n- Articles with multiple contradicted claims should be REJECTED\n- Only approve articles with \"high\" or \"medium\" reliability ratings\n\nREQUIREMENTS:\n1. Output must be a valid JSON object with:\n   - \"is_approved\": boolean (true if draft meets ALL criteria INCLUDING fact-checking)\n   - \"comments\": string with clear, constructive, actionable feedback\n\n2. If approved (is_approved: true), provide brief affirmation including fact-check confidence\n3. If not approved (is_approved: false), provide specific, actionable feedback:\n   - Factual errors that must be corrected (be specific about claims and evidence)\n   - Which principles or criteria need attention\n   - Content requirement violations (length, structure, format, etc.)\n   - Topic-specific improvements needed\n   - Suggestions for improvement\n\nIMPORTANT: \n- Use your specialized editorial expertise to provide domain-specific feedback\n- CRITICAL: Factual accuracy is non-negotiable - reject articles with significant factual errors\n- Check that ALL content requirements have been met (these were intelligently analyzed)\n- Be thorough but constructive, focusing on helping improve the draft\n- If content requirements specify minimum lengths, structural elements, or specific formats, ensure they are met\n- For factual issues, provide specific guidance on what needs to be verified/corrected\n\nReturn ONLY the JSON object, no additional text.\n\"\"\"\n        \n        try:\n            response = self.model.generate_content(prompt)\n            response_text = response.text.strip()\n            \n            # Clean the response to extract JSON\n            if response_text.startswith('```json'):\n                response_text = response_text[7:-3].strip()\n            elif response_text.startswith('```'):\n                response_text = response_text[3:-3].strip()\n            \n            data = json.loads(response_text)\n            \n            # Override approval if fact-checking fails\n            is_approved = data['is_approved']\n            if fact_check_report.overall_reliability == \"unreliable\" or has_critical_issues:\n                is_approved = False\n                if data['is_approved']:  # Was approved but fact-check failed\n                    data['comments'] += f\"\\n\\nFACT-CHECK OVERRIDE: Article rejected due to factual reliability issues. Overall reliability: {fact_check_report.overall_reliability}. Critical issues must be addressed before approval.\"\n            \n            return EditorReview(\n                is_approved=is_approved,\n                comments=data['comments']\n            ), fact_check_report\n            \n        except Exception as e:\n            print(f\"Error in CriticEditor: {e}\")\n            print(f\"Raw response: {response.text if 'response' in locals() else 'No response'}\")\n            \n            # Return a safe fallback with fact-check results\n            return EditorReview(\n                is_approved=False,\n                comments=f\"Error during editorial review: {e}. Additionally, fact-check reliability: {fact_check_report.overall_reliability}\"\n            ), fact_check_report\n    \n    def _format_fact_check_details(self, fact_check_report: FactCheckReport) -> str:\n        \"\"\"Format fact-check details for the editor prompt.\"\"\"\n        if not fact_check_report.verifications:\n            return \"No specific claims were fact-checked.\"\n        \n        details = \"\"\n        for i, verification in enumerate(fact_check_report.verifications[:10], 1):  # Limit to top 10 for prompt length\n            details += f\"{i}. CLAIM: {verification.claim.claim_text}\\n\"\n            details += f\"   STATUS: {verification.verification_status.upper()} (confidence: {verification.confidence:.2f})\\n\"\n            details += f\"   IMPORTANCE: {verification.claim.importance}\\n\"\n            if verification.verification_status == \"contradicted\":\n                details += f\"   ISSUE: {verification.explanation}\\n\"\n            details += \"\\n\"\n        \n        if len(fact_check_report.verifications) > 10:\n            details += f\"... and {len(fact_check_report.verifications) - 10} more claims checked.\\n\"\n        \n        return details\n\nprint(\"Enhanced CriticEditor with comprehensive fact-checking and intelligent requirement validation implemented successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Workflow Orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class DynamicBlogWriter:\n    def __init__(self, model, web_searcher):\n        self.model = model\n        self.web_searcher = web_searcher\n        self.requirement_analyzer = RequirementAnalyzer(model)\n        self.persona_architect = PersonaArchitect(model)\n        self.editor_persona_architect = EditorPersonaArchitect(model)\n        self.research_analyst = ResearchAnalyst(model, web_searcher)\n        self.content_synthesizer = ContentSynthesizer(model)\n        self.critic_editor = CriticEditor(model, web_searcher)  # Pass web_searcher for fact-checking\n        self.folder_name_generator = FolderNameGenerator(model)\n    \n    def _display_markdown_section(self, title: str, content: str, max_lines: int = None):\n        \"\"\"Display content as formatted markdown with optional truncation.\"\"\"\n        # Fix newline characters for proper markdown display\n        content = content.replace('\\\\n', '\\n').replace('\\\\t', '\\t')\n        \n        lines = content.split('\\n')\n        if max_lines and len(lines) > max_lines:\n            truncated_content = '\\n'.join(lines[:max_lines])\n            truncated_content += f\"\\n\\n*... (showing first {max_lines} lines of {len(lines)} total)*\"\n            display(Markdown(f\"### {title}\\n\\n{truncated_content}\"))\n        else:\n            display(Markdown(f\"### {title}\\n\\n{content}\"))\n    \n    def _display_search_results(self, search_results: List[SearchResult]):\n        \"\"\"Display search results in a formatted table.\"\"\"\n        if not search_results:\n            return\n        \n        content = f\"**Total Results:** {len(search_results)}\\n\\n\"\n        \n        for i, result in enumerate(search_results[:10], 1):  # Show top 10\n            content += f\"**{i}. {result.title}**\\n\"\n            content += f\"URL: {result.url}\\n\"\n            content += f\"Summary: {result.snippet}\\n\\n\"\n            content += \"---\\n\\n\"\n        \n        if len(search_results) > 10:\n            content += f\"*... and {len(search_results) - 10} more results*\"\n        \n        self._display_markdown_section(\"Real Web Search Results\", content, max_lines=30)\n    \n    def _display_fact_check_report(self, fact_check_report: FactCheckReport, cycle: int):\n        \"\"\"Display comprehensive fact-check results.\"\"\"\n        # Summary\n        summary = f\"\"\"**Fact-Check Summary for Review Cycle {cycle}**\n\n**Overall Reliability:** {fact_check_report.overall_reliability.upper()}\n**Total Claims Checked:** {fact_check_report.total_claims}\n**Verified:** {fact_check_report.verified_claims} | **Contradicted:** {fact_check_report.contradicted_claims} | **Insufficient Evidence:** {fact_check_report.insufficient_evidence_claims}\"\"\"\n        \n        if fact_check_report.critical_issues:\n            summary += f\"\\n\\n**CRITICAL ISSUES ({len(fact_check_report.critical_issues)}):**\\n\"\n            for issue in fact_check_report.critical_issues:\n                summary += f\"- {issue}\\n\"\n        else:\n            summary += f\"\\n\\n**No critical factual issues detected.**\"\n        \n        self._display_markdown_section(f\"Fact-Check Report {cycle}\", summary, max_lines=15)\n        \n        # Detailed claims (if any contradicted)\n        if fact_check_report.contradicted_claims > 0:\n            contradicted = [v for v in fact_check_report.verifications if v.verification_status == \"contradicted\"]\n            details = \"**CONTRADICTED CLAIMS REQUIRING ATTENTION:**\\n\\n\"\n            \n            for i, verification in enumerate(contradicted, 1):\n                details += f\"{i}. **{verification.claim.claim_text}**\\n\"\n                details += f\"   Priority: {verification.claim.importance.upper()}\\n\"\n                details += f\"   Issue: {verification.explanation}\\n\\n\"\n            \n            self._display_markdown_section(f\"Critical Fact-Check Issues {cycle}\", details, max_lines=20)\n    \n    def generate_blog_post(self, topic: str, style_and_background: str) -> Tuple[str, Path]:\n        \"\"\"\n        Enhanced workflow with intelligent requirement analysis and comprehensive fact-checking.\n        Returns: (final_blog_content, final_blog_path)\n        \"\"\"\n        print(f\"Starting Enhanced Dynamic Blog Writer with FACT-CHECKING for topic: '{topic}'\")\n        print(\"=\" * 70)\n        \n        # Initialize file manager with smart naming\n        file_manager = FileManager(topic, self.folder_name_generator)\n        \n        try:\n            # Phase 0: Intelligent Requirement Analysis\n            print(\"Phase 0: Analyzing and categorizing requirements intelligently...\")\n            requirement_analysis = self.requirement_analyzer.analyze_requirements(style_and_background, topic)\n            print(\"Requirements intelligently analyzed and categorized\")\n            \n            # Display requirement analysis with proper formatting\n            analysis_summary = f\"\"\"**Persona Requirements:**\n{requirement_analysis.persona_requirements}\n\n**Content Requirements:**\n{requirement_analysis.content_requirements}\n\n**Review Requirements:**\n{requirement_analysis.review_requirements}\"\"\"\n            \n            self._display_markdown_section(\"Intelligent Requirement Analysis\", analysis_summary, max_lines=20)\n            \n            # Save requirement analysis\n            file_manager.save_requirement_analysis(requirement_analysis, topic)\n            \n            # Phase 1: Persona Generation (Writer + Editor) using categorized requirements\n            print(\"Phase 1: Generating writer and editor personas with targeted requirements...\")\n            \n            # Generate writer persona using persona-specific requirements\n            persona_result = self.persona_architect.generate_persona_and_queries(topic, requirement_analysis.persona_requirements)\n            print(\"Generated writer persona using targeted persona requirements\")\n            \n            # Generate INDEPENDENT editor persona using review-specific requirements  \n            editor_result = self.editor_persona_architect.generate_editor_persona(topic, requirement_analysis.review_requirements)\n            print(\"Generated independent editor persona using targeted review requirements\")\n            \n            # Display writer persona\n            persona_text = str(persona_result.persona_prompt)\n            self._display_markdown_section(\"Writer Persona (From Persona Requirements)\", persona_text, max_lines=15)\n            \n            # Display editor persona\n            editor_text = str(editor_result.editor_persona)\n            self._display_markdown_section(\"Editor Persona (From Review Requirements)\", editor_text, max_lines=15)\n            \n            # Display search queries\n            queries_text = \"\\n\".join(f\"- {query}\" for query in persona_result.search_queries)\n            self._display_markdown_section(\"Generated Search Queries\", queries_text)\n            \n            # Save personas to files\n            file_manager.save_persona_details(persona_text, topic)\n            file_manager.save_editor_persona_details(editor_text, editor_result.review_criteria, topic)\n            \n            # Phase 2: Real Web Research\n            print(\"\\nPhase 2: Conducting REAL web research...\")\n            research_result = self.research_analyst.conduct_research(persona_result.search_queries)\n            print(f\"Research completed - {research_result.source_count} real web sources\")\n            \n            # Display actual search results\n            if research_result.search_results:\n                self._display_search_results(research_result.search_results)\n                file_manager.save_search_results(research_result.search_results)\n            \n            # Display research analysis\n            research_content = research_result.content\n            if \"--- RESEARCH START ---\" in research_content and \"--- RESEARCH END ---\" in research_content:\n                start_idx = research_content.find(\"--- RESEARCH START ---\") + len(\"--- RESEARCH START ---\")\n                end_idx = research_content.find(\"--- RESEARCH END ---\")\n                clean_research = research_content[start_idx:end_idx].strip()\n            else:\n                clean_research = research_content\n            \n            source_info = f\"**Real Web Sources:** {research_result.source_count}\\n\\n{clean_research}\"\n            self._display_markdown_section(\"Research Analysis\", source_info, max_lines=25)\n            \n            # Phase 3: Content Generation & Review Loop with FACT-CHECKING\n            print(\"\\nPhase 3: Starting content generation with COMPREHENSIVE FACT-CHECKING...\")\n            \n            current_draft = None\n            review_cycle = 0\n            max_cycles = 3\n            all_reviews = []\n            all_fact_checks = []\n            \n            while review_cycle < max_cycles:\n                review_cycle += 1\n                print(f\"\\nReview Cycle {review_cycle}/{max_cycles}\")\n                \n                # Generate or revise draft using content-specific requirements\n                if current_draft is None:\n                    print(\"  - Writing initial draft using targeted content requirements...\")\n                    draft = self.content_synthesizer.write_blog_post(\n                        persona_text,\n                        research_result.content,\n                        topic,\n                        requirement_analysis.content_requirements  # Use targeted content requirements\n                    )\n                else:\n                    print(\"  - Revising draft based on editorial AND fact-check feedback...\")\n                    draft = self.content_synthesizer.write_blog_post(\n                        persona_text,\n                        research_result.content,\n                        topic,\n                        requirement_analysis.content_requirements,  # Use targeted content requirements\n                        editorial_feedback=last_review.comments\n                    )\n                \n                # Save draft\n                draft_path = file_manager.save_draft(draft.content, review_cycle)\n                current_draft = draft\n                \n                # Display draft with word count\n                word_count = self.critic_editor._count_words(draft.content)\n                draft_preview = f\"**Word Count:** {word_count} words\\n\\n{draft.content}\"\n                self._display_markdown_section(f\"Draft {review_cycle}\", draft_preview, max_lines=25)\n                \n                # Review draft WITH COMPREHENSIVE FACT-CHECKING\n                print(\"  - Reviewing draft with specialized editor and COMPREHENSIVE FACT-CHECKING...\")\n                review, fact_check_report = self.critic_editor.review_draft(\n                    draft.content, \n                    research_result.content,\n                    editor_result.editor_persona,\n                    editor_result.review_criteria,\n                    requirement_analysis.content_requirements,  # Use targeted content requirements for validation\n                    topic\n                )\n                \n                # Save both review and fact-check report\n                review_path = file_manager.save_review(review, review_cycle)\n                fact_check_path = file_manager.save_fact_check_report(fact_check_report, review_cycle)\n                \n                all_reviews.append((review_cycle, review))\n                all_fact_checks.append((review_cycle, fact_check_report))\n                \n                # Display comprehensive review results\n                review_content = f\"**Approved:** {'YES' if review.is_approved else 'NO'}\\n\"\n                review_content += f\"**Fact-Check Reliability:** {fact_check_report.overall_reliability.upper()}\\n\\n\"\n                review_content += f\"**Editorial Feedback:**\\n{review.comments}\"\n                self._display_markdown_section(f\"Editorial Review {review_cycle}\", review_content)\n                \n                # Display detailed fact-check results\n                self._display_fact_check_report(fact_check_report, review_cycle)\n                \n                if review.is_approved:\n                    print(f\"Draft approved after {review_cycle} cycle(s) with fact-check reliability: {fact_check_report.overall_reliability}\")\n                    break\n                else:\n                    if fact_check_report.overall_reliability in [\"unreliable\", \"low\"]:\n                        print(\"Draft needs revision due to FACTUAL ISSUES...\")\n                    else:\n                        print(\"Draft needs revision due to editorial feedback...\")\n                    last_review = review\n                    \n                    if review_cycle == max_cycles:\n                        print(f\"Maximum review cycles ({max_cycles}) reached. Using last draft.\")\n                        print(f\"FINAL FACT-CHECK STATUS: {fact_check_report.overall_reliability}\")\n            \n            # Phase 4: Finalization with Final Assessment\n            print(\"\\nPhase 4: Finalizing blog post with final fact-check assessment...\")\n            final_path = file_manager.save_final_blog(current_draft.content)\n            \n            # Display final blog post with word count\n            final_word_count = self.critic_editor._count_words(current_draft.content)\n            final_preview = f\"**Final Word Count:** {final_word_count} words\\n\\n{current_draft.content}\"\n            self._display_markdown_section(\"Final Blog Post\", final_preview)\n            \n            # Final fact-check status\n            final_fact_check = all_fact_checks[-1][1] if all_fact_checks else None\n            \n            # Display comprehensive summary with fact-checking\n            summary = f\"\"\"**Topic:** {topic}\n**Smart Folder:** {file_manager.folder_name}\n**Review Cycles:** {review_cycle}\n**Final Status:** {'Approved' if all_reviews and all_reviews[-1][1].is_approved else 'Max cycles reached'}\n**FACT-CHECK RELIABILITY:** {final_fact_check.overall_reliability.upper() if final_fact_check else 'Unknown'}\n**Final Word Count:** {final_word_count} words\n**Real Web Sources:** {research_result.source_count}\n**Search Queries Used:** {len(persona_result.search_queries)}\n\n**FACT-CHECKING SUMMARY:**\n  - Claims Verified: {final_fact_check.verified_claims if final_fact_check else 0}\n  - Claims Contradicted: {final_fact_check.contradicted_claims if final_fact_check else 0}\n  - Critical Issues: {len(final_fact_check.critical_issues) if final_fact_check else 0}\n  \n**Intelligence Features:**\n  - Requirement Analysis: AI-categorized requirements\n  - Targeted Persona: Generated from persona-specific requirements\n  - Specialized Editor: Created from review-specific requirements  \n  - Smart Content: Uses content-specific requirements\n  - COMPREHENSIVE FACT-CHECKING: Real-time verification of all factual claims\n  \n**Output Folder:** `{file_manager.output_dir}`\n**Final Blog File:** `{final_path}`\n\n**Research Quality:** Real-time web search results\n**Fact-Check Quality:** Comprehensive online verification of all claims\n**Intelligence Level:** Advanced requirement categorization and fact verification\n**Reliability Assurance:** {final_fact_check.overall_reliability.upper() if final_fact_check else 'Unknown'} factual reliability\"\"\"\n            \n            self._display_markdown_section(\"Enhanced Generation Summary with Fact-Checking\", summary)\n            \n            print(\"\\nBlog generation completed with COMPREHENSIVE FACT-CHECKING!\")\n            print(f\"CRITICAL: Final fact-check reliability is {final_fact_check.overall_reliability.upper() if final_fact_check else 'Unknown'}\")\n            if final_fact_check and final_fact_check.critical_issues:\n                print(f\"WARNING: {len(final_fact_check.critical_issues)} critical factual issues detected\")\n            \n            return current_draft.content, final_path\n            \n        except Exception as e:\n            print(f\"\\nError during blog generation: {e}\")\n            print(\"Please check your API keys and internet connection.\")\n            raise\n\nprint(\"COMPREHENSIVE DynamicBlogWriter with FACT-CHECKING and AI-powered requirement analysis implemented successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize the Dynamic Blog Writer system\nif USE_FREE_SEARCH:\n    # Free web search - no additional API keys needed\n    web_searcher = FreeWebSearcher()\n    search_method = \"FREE DuckDuckGo search\"\nelse:\n    # Google Custom Search API\n    web_searcher = WebSearcher(GOOGLE_CSE_API_KEY, GOOGLE_CSE_ID)\n    search_method = \"Google Custom Search API\"\n\nblog_writer = DynamicBlogWriter(model, web_searcher)\n\nprint(\"Dynamic Blog Writer system initialized and ready!\")\nprint(f\"Search method: {search_method}\")\nif USE_FREE_SEARCH:\n    print(\"Benefits: No additional API keys required, completely free!\")\nelse:\n    print(\"Benefits: Higher quality results, 100 free searches/day\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example usage - uncomment and modify as needed\ntopic = \"The Future of Artificial Intelligence in Healthcare\"\nstyle_and_background = \"\"\"\nTarget audience: General tech-savvy readers interested in healthcare innovation\nTone: Professional but accessible, optimistic yet balanced\nStyle: Informative with real-world examples, approximately 500 words\nBackground: Write from the perspective of someone knowledgeable about both AI and healthcare trends\n\"\"\"\n\n# Uncomment the following lines to run the system:\n# final_content, final_path = blog_writer.generate_blog_post(topic, style_and_background)\n# print(f\"\\nFinal blog post saved to: {final_path}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Documentation\n\n### Project Overview\nThis Dynamic Micro-Blog Writer implements a complete multi-agent system for autonomous blog generation with **COMPREHENSIVE FACT-CHECKING** and the following features:\n\n#### Implemented Features\n- **Persona Architect**: Generates writer personas and search queries from user input\n- **Research Analyst**: Performs real web searches and consolidates findings  \n- **Content Synthesizer**: Writes blog posts following persona and research\n- **Critic/Editor**: Reviews drafts with COMPREHENSIVE FACT-CHECKING against quality principles\n- **FACT-CHECKING SYSTEM**: Real-time verification of all factual claims via web search\n- **Iterative Review Loop**: Up to 3 cycles of draft revision based on editorial AND fact-check feedback\n- **File Management**: Automatic folder creation with timestamped naming and fact-check reports\n- **Error Handling**: Comprehensive error handling with informative messages\n- **Google Colab Integration**: Secure API key management using Colab secrets\n\n#### NEW: Comprehensive Fact-Checking System\n- **Claim Extraction**: AI identifies verifiable claims (statistics, dates, names, facts, quotes)\n- **Evidence Search**: Real-time web searches for each claim using targeted queries\n- **Verification Analysis**: LLM compares claims against search evidence\n- **Reliability Scoring**: Overall article reliability (high/medium/low/unreliable)\n- **Critical Issue Detection**: Identifies contradicted claims that must be corrected\n- **Detailed Reports**: Comprehensive fact-check reports with evidence sources\n\n#### Quality Principles (P1-P5)\n1. **Evidentiary Support**: All claims traceable to research material\n2. **Clarity and Conciseness**: Precise, unambiguous writing\n3. **Engaging Narrative**: Strong hook, logical flow, memorable conclusion\n4. **Structural Integrity**: Clear title, introduction, body, conclusion\n5. **Intellectual Honesty**: Accurate information representation\n\n#### CRITICAL: Fact-Checking Requirements\n- **Articles with \"unreliable\" fact-check rating are REJECTED**\n- **Articles with contradicted high-priority claims are REJECTED**\n- **Only articles with \"high\" or \"medium\" reliability are approved**\n- **All factual claims are verified against real-time web search**\n\n#### Usage Instructions for Google Colab\n1. **Setup Secrets**: Add `GEMINI_API_KEY` to Colab secrets (key icon in sidebar)\n2. **Run Cells**: Execute all cells in order to initialize the system\n3. **Customize**: Modify the example topic and style guide as needed\n4. **Execute**: Uncomment and run the test function or main usage example\n5. **Review**: Check fact-check reports for reliability assessment\n6. **Download**: Use Colab's file browser to download generated content\n\n#### Enhanced Output Structure (in Colab filesystem)\n```\n/content/YYYYMMDD_topic_abbreviation/\n├── requirement_analysis.md (NEW: AI-categorized requirements)\n├── writer_persona.md\n├── editor_persona.md (NEW: Independent editor persona)\n├── search_results.md (Real web search results)\n├── draft_1.md\n├── review_1.md\n├── fact_check_1.md (NEW: Comprehensive fact-check report)\n├── draft_2.md (if revision needed)\n├── review_2.md (if revision needed)\n├── fact_check_2.md (NEW: Updated fact-check report)\n├── draft_3.md (if second revision needed)\n├── review_3.md (if second revision needed)\n├── fact_check_3.md (NEW: Final fact-check report)\n└── final_blog.md\n```\n\n#### Google Colab Security Features\n- **Secure API Storage**: API keys stored in encrypted Colab secrets\n- **No Hardcoded Keys**: No sensitive information in notebook code\n- **Access Control**: Secrets only accessible when explicitly enabled\n- **Session Isolation**: Keys don't persist between sessions\n\n#### Enhanced Acceptance Criteria Met\n- AC-1: User can run workflow with topic and style guide only\n- AC-2: All four agents execute as defined in requirements\n- AC-3: Iterative review loop with draft/review saving\n- AC-4: Proper termination after approval or 3 cycles\n- AC-5: Correct file generation in uniquely named folders\n- AC-6: Coherent, stylistically appropriate, factually consistent output\n- **NEW**: Secure integration with Google Colab secrets management\n- **NEW**: COMPREHENSIVE fact-checking with real-time verification\n- **NEW**: AI-powered requirement analysis and categorization\n- **NEW**: Independent editor personas with specialized criteria\n\n#### CRITICAL SAFEGUARDS\n- **Factual Accuracy**: Non-negotiable requirement - articles with factual errors are rejected\n- **Evidence-Based**: All claims must be verified against real web sources\n- **Transparency**: Detailed fact-check reports show verification status of each claim\n- **Reliability Assurance**: Only high/medium reliability articles are approved\n\n**Ready for Google Colab with COMPREHENSIVE FACT-CHECKING! Simply add your API key to secrets and run all cells.**"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Documentation\n\n### Project Overview\nThis Dynamic Micro-Blog Writer implements a complete multi-agent system for autonomous blog generation with the following features:\n\n#### Implemented Features\n- **Persona Architect**: Generates writer personas and search queries from user input\n- **Research Analyst**: Simulates web research and consolidates findings  \n- **Content Synthesizer**: Writes 500-word blog posts following persona and research\n- **Critic/Editor**: Reviews drafts against quality principles and provides feedback\n- **Iterative Review Loop**: Up to 3 cycles of draft revision based on editorial feedback\n- **File Management**: Automatic folder creation with timestamped naming convention\n- **Error Handling**: Comprehensive error handling with informative messages\n- **Google Colab Integration**: Secure API key management using Colab secrets\n\n#### Quality Principles (P1-P5)\n1. **Evidentiary Support**: All claims traceable to research material\n2. **Clarity and Conciseness**: Precise, unambiguous writing\n3. **Engaging Narrative**: Strong hook, logical flow, memorable conclusion\n4. **Structural Integrity**: Clear title, introduction, body, conclusion\n5. **Intellectual Honesty**: Accurate information representation\n\n#### Usage Instructions for Google Colab\n1. **Setup Secrets**: Add `GEMINI_API_KEY` to Colab secrets (key icon in sidebar)\n2. **Run Cells**: Execute all cells in order to initialize the system\n3. **Customize**: Modify the example topic and style guide as needed\n4. **Execute**: Uncomment and run the test function or main usage example\n5. **Download**: Use Colab's file browser to download generated content\n\n#### Output Structure (in Colab filesystem)\n```\n/content/YYYYMMDD_topic_abbreviation/\n├── writer_persona.md\n├── draft_1.md\n├── review_1.md\n├── draft_2.md (if revision needed)\n├── review_2.md (if revision needed)\n├── draft_3.md (if second revision needed)\n├── review_3.md (if second revision needed)\n└── final_blog.md\n```\n\n#### Google Colab Security Features\n- **Secure API Storage**: API keys stored in encrypted Colab secrets\n- **No Hardcoded Keys**: No sensitive information in notebook code\n- **Access Control**: Secrets only accessible when explicitly enabled\n- **Session Isolation**: Keys don't persist between sessions\n\n#### Acceptance Criteria Met\n- AC-1: User can run workflow with topic and style guide only\n- AC-2: All four agents execute as defined in requirements\n- AC-3: Iterative review loop with draft/review saving\n- AC-4: Proper termination after approval or 3 cycles\n- AC-5: Correct file generation in uniquely named folders\n- AC-6: Coherent, stylistically appropriate, factually consistent output\n- **NEW**: Secure integration with Google Colab secrets management\n\n**Ready for Google Colab! Simply add your API key to secrets and run all cells.**"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}